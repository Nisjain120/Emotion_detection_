{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1b5s_e8sPwja6mLf4520pRNf52awYjzDx",
      "authorship_tag": "ABX9TyPhUbzxWS+G+ehm2MEAQfpa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nisjain120/Emotion_detection_/blob/main/Emotion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q \"/content/archive (18).zip\" -d /content/newdata\n"
      ],
      "metadata": {
        "id": "h-8nTRQ0K2zi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mobileNet"
      ],
      "metadata": {
        "id": "E_CFzKxmVQuO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the number of classes in your dataset\n",
        "num_classes = 7  # Update this value to match your dataset (7 classes based on earlier observations)\n",
        "\n",
        "# Load pre-trained MobileNetV2 with frozen layers\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Fine-tune: Unfreeze some layers in the base model\n",
        "for layer in base_model.layers[-30:]:  # Unfreeze the last 30 layers for fine-tuning\n",
        "    layer.trainable = True\n",
        "\n",
        "# Add custom classification layers\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),  # Replaces Flatten for better performance\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')  # Adjusted num_classes to 7\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Data Generators with resizing and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "# Load datasets\n",
        "train_data = train_datagen.flow_from_directory(\n",
        "    '/content/newdata/archive (17)/train',  # Training data path\n",
        "    target_size=(224, 224),  # Resize to match MobileNetV2\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(\n",
        "    '/content/newdata/archive (17)/valid',  # Validation data path\n",
        "    target_size=(224, 224),  # Resize to match MobileNetV2\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_data = test_datagen.flow_from_directory(\n",
        "    '/content/newdata/archive (17)/test',  # Testing data path\n",
        "    target_size=(224, 224),  # Resize to match MobileNetV2\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Callbacks\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=valid_data,\n",
        "    epochs=15,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Test Loss: {test_loss:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdooD97IVQwZ",
        "outputId": "5c31679c-fb0f-46f9-9942-ef9dc8fb678b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17729 images belonging to 7 classes.\n",
            "Found 4833 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "Epoch 1/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 436ms/step - accuracy: 0.3518 - loss: 1.6579 - val_accuracy: 0.1682 - val_loss: 8.0515 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 392ms/step - accuracy: 0.5141 - loss: 1.3072 - val_accuracy: 0.4053 - val_loss: 2.6464 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 390ms/step - accuracy: 0.5343 - loss: 1.2466 - val_accuracy: 0.3323 - val_loss: 2.9769 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 393ms/step - accuracy: 0.5658 - loss: 1.1720 - val_accuracy: 0.3224 - val_loss: 3.7596 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.5819 - loss: 1.1391\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m219s\u001b[0m 391ms/step - accuracy: 0.5819 - loss: 1.1391 - val_accuracy: 0.4258 - val_loss: 2.8278 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 393ms/step - accuracy: 0.6028 - loss: 1.0893 - val_accuracy: 0.5564 - val_loss: 1.3355 - learning_rate: 5.0000e-04\n",
            "Epoch 7/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 409ms/step - accuracy: 0.6168 - loss: 1.0483 - val_accuracy: 0.5510 - val_loss: 1.3357 - learning_rate: 5.0000e-04\n",
            "Epoch 8/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 404ms/step - accuracy: 0.6240 - loss: 1.0143 - val_accuracy: 0.5651 - val_loss: 1.2566 - learning_rate: 5.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 388ms/step - accuracy: 0.6514 - loss: 0.9734 - val_accuracy: 0.5522 - val_loss: 1.3128 - learning_rate: 5.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 393ms/step - accuracy: 0.6314 - loss: 1.0087 - val_accuracy: 0.5736 - val_loss: 1.1907 - learning_rate: 5.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 392ms/step - accuracy: 0.6517 - loss: 0.9446 - val_accuracy: 0.6023 - val_loss: 1.0748 - learning_rate: 5.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m222s\u001b[0m 395ms/step - accuracy: 0.6555 - loss: 0.9291 - val_accuracy: 0.6007 - val_loss: 1.1175 - learning_rate: 5.0000e-04\n",
            "Epoch 13/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m220s\u001b[0m 393ms/step - accuracy: 0.6757 - loss: 0.8991 - val_accuracy: 0.5901 - val_loss: 1.1850 - learning_rate: 5.0000e-04\n",
            "Epoch 14/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 382ms/step - accuracy: 0.6752 - loss: 0.8877\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 391ms/step - accuracy: 0.6752 - loss: 0.8878 - val_accuracy: 0.5003 - val_loss: 1.5114 - learning_rate: 5.0000e-04\n",
            "Epoch 15/15\n",
            "\u001b[1m555/555\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 392ms/step - accuracy: 0.6961 - loss: 0.8308 - val_accuracy: 0.6418 - val_loss: 0.9986 - learning_rate: 2.5000e-04\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 69ms/step - accuracy: 0.6399 - loss: 1.0212\n",
            "Test Accuracy: 0.64\n",
            "Test Loss: 1.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import os\n",
        "\n",
        "# Save the trained model\n",
        "model.save('emotion_detection_model.h5')  # Save the model for future use\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = model.evaluate(test_data)\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
        "print(f\"Test Loss: {test_loss:.2f}\")\n",
        "\n",
        "# Function to predict an emotion from a single image\n",
        "def predict_emotion(image_path, model_path='emotion_detection_model.h5', target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Predicts the emotion for a given image.\n",
        "\n",
        "    Parameters:\n",
        "    - image_path: Path to the image to predict.\n",
        "    - model_path: Path to the saved model.\n",
        "    - target_size: Target size for the image (default is (224, 224)).\n",
        "\n",
        "    Returns:\n",
        "    - Predicted class label and probabilities.\n",
        "    \"\"\"\n",
        "    # Check if the file extension is supported\n",
        "    supported_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "    file_extension = os.path.splitext(image_path)[1].lower()\n",
        "    if file_extension not in supported_extensions:\n",
        "        raise ValueError(f\"Unsupported file extension: {file_extension}. Supported extensions: {supported_extensions}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ1QLOQWVQym",
        "outputId": "35f051b5-f63c-4c5b-e4b2-4c397abf4583"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 36ms/step - accuracy: 0.6369 - loss: 1.0438\n",
            "Test Accuracy: 0.64\n",
            "Test Loss: 1.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_emotion(image_path, model_path=r'/content/emotion_detection_model.h5', target_size=(224, 224)):\n",
        "\n",
        "    # Load the saved model\n",
        "    model = load_model(r'/content/emotion_detection_model.h5')\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    try:\n",
        "        image = load_img(image_path, target_size=target_size)  # Load image and resize\n",
        "        image_array = img_to_array(image) / 255.0  # Normalize pixel values\n",
        "        image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error loading or preprocessing image: {e}\")\n",
        "\n",
        "    # Make a prediction\n",
        "    predictions = model.predict(image_array)\n",
        "    predicted_class = np.argmax(predictions)  # Get the class with the highest probability\n",
        "\n",
        "    return predicted_class, predictions\n",
        "\n",
        "# Example usage\n",
        "# Upload your image and use its path for prediction\n",
        "image_path = r'/content/KA.FE1.45.tiff'  # Replace with the actual image path after upload\n",
        "try:\n",
        "    predicted_class, predictions = predict_emotion(image_path)\n",
        "    # Display the results\n",
        "    class_labels = list(train_data.class_indices.keys())  # Get class labels from the training data\n",
        "    print(f\"Predicted Class: {class_labels[predicted_class]}\")\n",
        "    print(f\"Prediction Probabilities: {predictions}\")\n",
        "except ValueError as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdRM5woUVQ04",
        "outputId": "0f12ad22-3b04-46d5-b50e-10691ac259d1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
            "Predicted Class: sad\n",
            "Prediction Probabilities: [[2.3606446e-02 1.3553229e-04 1.3167349e-01 1.0594956e-03 1.1530323e-01\n",
            "  7.2770417e-01 5.1772455e-04]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wEyrM92VQ3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P1f5mgqmVQ6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EDxYhL_oVQ9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}